{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Доработать приложение по поиску авиабилетов, чтобы оно возвращало билеты по названию города, а не по IATA коду. (У aviasales есть для этого дополнительное API) Пункт отправления и пункт назначения должны передаваться в качестве параметров. Сделать форматированный вывод, который содержит в себе пункт отправления, пункт назначения, дату вылета, цену билета (можно добавить еще другие параметры по желанию)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:55:13.557806Z",
     "start_time": "2019-08-11T12:55:10.670972Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:55:13.568705Z",
     "start_time": "2019-08-11T12:55:13.559728Z"
    }
   },
   "outputs": [],
   "source": [
    "# # %load aviasales.py\n",
    "# from pprint import pprint\n",
    "# import requests\n",
    "# import json\n",
    "# service = 'http://min-prices.aviasales.ru/calendar_preload?'\n",
    "# fromCity = 'MOW'\n",
    "# toCity = 'LED'\n",
    "# link = f'{service}origin={fromCity}&destination={toCity}&one_way=true'\n",
    "# req = requests.get(link)\n",
    "# data = json.loads(req.text)\n",
    "# for i in data['best_prices']:\n",
    "#     print(i['value'],i['depart_date'],i['return_date'],i['gate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T12:55:13.602613Z",
     "start_time": "2019-08-11T12:55:13.576683Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cities():\n",
    "    \n",
    "    link = 'http://api.travelpayouts.com/data/ru/cities.json'\n",
    "    \n",
    "    with requests.session() as s:\n",
    "        r = s.get(link)\n",
    "        \n",
    "    data = json.loads(r.text)\n",
    "    \n",
    "    cities = dict()\n",
    "    \n",
    "    for item in data:\n",
    "        cities[item['name']] = item['code']\n",
    "        cities[item['name_translations']['en']] = item['code']\n",
    "        \n",
    "    return cities\n",
    "\n",
    "    \n",
    "def get_tickets(origin_name, destination_name, one_way_ticket=True):\n",
    "    \n",
    "    cities = get_cities()\n",
    "\n",
    "    origin = cities.get(origin_name, '--')\n",
    "    destination = cities.get(destination_name, '--')\n",
    "    \n",
    "    ticket_type = {True: 'true', False: 'false'}\n",
    "    \n",
    "    one_way = ticket_type.get(one_way_ticket, True)\n",
    "    \n",
    "    service = 'http://min-prices.aviasales.ru/calendar_preload?'\n",
    "\n",
    "    link = f'{service}origin={origin}&destination={destination}&one_way={one_way}'\n",
    "\n",
    "    with requests.session() as s:\n",
    "        r = s.get(link)\n",
    "\n",
    "    data = json.loads(r.text)\n",
    "    \n",
    "    feature_names = ['origin', 'destination', 'gate', 'depart_date', 'return_date', 'number_of_changes', 'value']\n",
    "    \n",
    "    tickets = pd.DataFrame(data['best_prices'])[feature_names]\n",
    "    tickets['origin'] = origin_name\n",
    "    tickets['destination'] = destination_name\n",
    "    \n",
    "    for idx, row in tickets.iterrows():\n",
    "    \n",
    "        print('*' * 42)\n",
    "        print(f'Вариант №:             {idx+1}')\n",
    "        print(f'Пункт отправления:     {row[0]}')\n",
    "        print(f'Пункт назначения:      {row[1]}')\n",
    "        print(f'Продавец:              {row[2]}')\n",
    "        print(f'Дата отправления:      {row[3]}')\n",
    "        print(f'Дата возвращения:      {row[4]}')\n",
    "        print(f'Количество пересадок:  {row[5]}')\n",
    "        print(f'Цена билета:           {row[6]}')\n",
    "        print('*' * 42)\n",
    "        print('\\n')\n",
    "\n",
    "    return tickets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T12:55:10.681Z"
    }
   },
   "outputs": [],
   "source": [
    "tickets = get_tickets('Москва', 'Волгоград', one_way_ticket=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T12:55:10.684Z"
    }
   },
   "outputs": [],
   "source": [
    "tickets.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. В приложении парсинга википедии получить первую ссылку из раздела \"Ссылки\" и вывести все значимые слова из неё. Результат записать в файл в форматированном виде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T12:55:10.686Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T12:55:10.687Z"
    }
   },
   "outputs": [],
   "source": [
    "# # %load wiki.py\n",
    "# from pprint import pprint\n",
    "# import requests\n",
    "# import re\n",
    "\n",
    "# def get_link(topic):\n",
    "#     link='https://ru.wikipedia.org/wiki/'+topic.capitalize()\n",
    "#     return link\n",
    "\n",
    "# def get_topic_page(topic):\n",
    "#     link = get_link(topic)\n",
    "#     html = requests.get(link).text\n",
    "#     return html\n",
    "\n",
    "# def get_topic_text(topic):\n",
    "#     html_content = get_topic_page(topic)\n",
    "#     words = re.findall(\"[а-яА-Я]{3,}\",html_content)\n",
    "#     #text = ' '.join(words)\n",
    "#     return words\n",
    "\n",
    "# # text = get_topic_text('Дерево')\n",
    "# # print(len(text))\n",
    "# # print(text[0:1000])\n",
    "\n",
    "# def get_common_words(topic):\n",
    "#     words_list = get_topic_text(topic)\n",
    "#     rate={}\n",
    "#     for word in words_list:\n",
    "#         if word in rate:\n",
    "#             rate[word]+=1\n",
    "#         else:\n",
    "#             rate[word]=1\n",
    "#     rate_list = list(rate.items())\n",
    "#     rate_list.sort(key = lambda x: -x[1])\n",
    "#     return rate_list\n",
    "\n",
    "# dict1 = get_common_words('Россия')\n",
    "# pprint(dict1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T12:55:10.689Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_link(topic):\n",
    "    link='https://ru.wikipedia.org/wiki/'+topic.capitalize()\n",
    "    return link\n",
    "\n",
    "def get_topic_page(topic):\n",
    "    link = get_link(topic)\n",
    "    html = requests.get(link).text\n",
    "    return html\n",
    "\n",
    "def get_first_linked_page(topic, n_words=10):\n",
    "    html_content = get_topic_page(topic)\n",
    "    try:\n",
    "        text = re.findall(r'<li><a rel=.+\\sclass=.+\\shref=\\\"https?://\\S+?\\\">', html_content)\n",
    "        for idx, line in enumerate(text[:1]):\n",
    "            link = re.findall(r'\"((http)s?://.*?)\"', line)[0][0]\n",
    "            try:\n",
    "                html = requests.get(link, timeout=5).text\n",
    "                words = re.findall(r'[а-яА-Я]{3,}', html)\n",
    "                counter = Counter(words)\n",
    "                most_common_words = dict(counter.most_common(n_words))\n",
    "                file_name = topic.capitalize() + '_link_' + str(idx+1) + '.json'\n",
    "                with open(file_name, 'w', encoding='utf-8') as f:\n",
    "                    f.write(link)\n",
    "                with open(file_name, 'a', encoding='utf-8') as f:\n",
    "                    json.dump(most_common_words, f, ensure_ascii=False)\n",
    "                print(f'Ссылка №{idx+1}: {link}')\n",
    "                print(f'{n_words} значимых слов:')\n",
    "                pprint(most_common_words)\n",
    "                print(f'Data saved to: {file_name}')\n",
    "                print('\\n')\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T12:55:10.692Z"
    }
   },
   "outputs": [],
   "source": [
    "get_first_linked_page('Москва')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.* Научить приложение определять количество ссылок в статье (раздел Ссылки). Выполнить поиск слов в статьях по каждой ссылке и результаты записать в отдельные файлы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T12:55:10.694Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_linked_pages(topic, n_words=10):\n",
    "    html_content = get_topic_page(topic)\n",
    "    try:\n",
    "        text = re.findall(r'<li><a rel=.+\\sclass=.+\\shref=\\\"https?://\\S+?\\\">', html_content)\n",
    "        print(f'Кол-во ссылок в статье по теме {topic}: {len(text)}')\n",
    "        print('\\n')\n",
    "        for idx, line in enumerate(text):\n",
    "            link = re.findall(r'\"((http)s?://.*?)\"', line)[0][0]\n",
    "            try:\n",
    "                html = requests.get(link, timeout=5).text\n",
    "                words = re.findall(r'[а-яА-Я]{3,}', html)\n",
    "                counter = Counter(words)\n",
    "                most_common_words = dict(counter.most_common(n_words))\n",
    "                file_name = topic.capitalize() + '_link_' + str(idx+1) + '.json'\n",
    "                with open(file_name, 'w', encoding='utf-8') as f:\n",
    "                    f.write(link)\n",
    "                with open(file_name, 'a', encoding='utf-8') as f:\n",
    "                    json.dump(most_common_words, f, ensure_ascii=False)\n",
    "                print(f'Ссылка №{idx+1}: {link}')\n",
    "                print(f'{n_words} значимых слов:')\n",
    "                pprint(most_common_words)\n",
    "                print(f'Data saved to: {file_name}')\n",
    "                print('\\n')\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-11T12:55:10.697Z"
    }
   },
   "outputs": [],
   "source": [
    "get_all_linked_pages('Москва')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
